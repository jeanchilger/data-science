{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce5e12a",
   "metadata": {},
   "source": [
    "Link to the course: https://huggingface.co/course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89769fab",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591ec55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "793f3f58",
   "metadata": {},
   "source": [
    "# 1. Transformers Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42045a84",
   "metadata": {},
   "source": [
    "The `pipeline` object from transformers library connects a model with all necessary preprocessing and postprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b02a8",
   "metadata": {},
   "source": [
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71806161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "2021-10-16 22:45:08.263901: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-16 22:45:08.264575: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-16 22:45:08.267449: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-16 22:45:08.284707: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(classifier(\"I've been waiting for a HuggingFace course my whole life.\"))\n",
    "print(classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10281c",
   "metadata": {},
   "source": [
    "A list with available pipelines may be found [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0840b82",
   "metadata": {},
   "source": [
    "## How do Transformers work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ae650",
   "metadata": {},
   "source": [
    "Language models, self-supervised learning, transfer learning.\n",
    "\n",
    "Transformers are **language models**. This type of model develops a statiscal understanding of the language it was tranined on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b57de",
   "metadata": {},
   "source": [
    "## Encoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaade51",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder part of the transformer. Such models are best suited for tasks that require a knowledge of the whole sentence (eg. NER, sentence classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce3c05",
   "metadata": {},
   "source": [
    "## Decoder Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe4925",
   "metadata": {},
   "source": [
    "Decoder models use only de decoder part of the transformer. At each stage, they have access only to the words preceding the current word. These models are often called auto-regressive models. Such models are best suited for tasks that involves text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5c95c",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Models (Sequence to Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2f2d2",
   "metadata": {},
   "source": [
    "These models use both parts of the transformer architecture. Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input (eg. summarization, translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b2adfe",
   "metadata": {},
   "source": [
    "## Limitations and Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed75327",
   "metadata": {},
   "source": [
    "In the above code, note how the models generates professions skewed based on gender. For instance \"carpenter\" is often associated with a men's work while \"nurse\" is associated with a wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf7d88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396319fbc4264dca90f278c93518e9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a271635ad24f61a99a53859d48e006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-17 21:20:50.200597: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-17 21:20:50.201160: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-17 21:20:50.203377: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564ebe6e25464cf8b2637a67991b02d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bff32aa02d241a59647a40575996a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10f82785f1e4c0bb9367c96730d9728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e933794",
   "metadata": {},
   "source": [
    "# 2. Using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3829113",
   "metadata": {},
   "source": [
    " # 3. Fine-tuning a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac066b",
   "metadata": {},
   "source": [
    "# 4 Sharing models and tokenizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
